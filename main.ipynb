{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb072e1",
   "metadata": {},
   "source": [
    "# Projeto Aplicado III - Construindo um sistema de recomenda√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c42892",
   "metadata": {},
   "source": [
    "O projeto tem como objetivo **desenvolver um sistema de recomenda√ß√£o de filmes** utilizando o *5000 Movie Dataset*, dispon√≠vel no Kaggle, que re√∫ne informa√ß√µes detalhadas sobre cerca de 5.000 produ√ß√µes do The Movie Database (TMDb). Esse conjunto de dados inclui vari√°veis como **or√ßamento, g√™neros, popularidade, empresas produtoras, pa√≠ses de produ√ß√£o, elenco e equipe t√©cnica**, permitindo a aplica√ß√£o de t√©cnicas de aprendizado de m√°quina para sugerir conte√∫dos mais relevantes aos usu√°rios.\n",
    "\n",
    "A iniciativa busca n√£o apenas aprimorar compet√™ncias pr√°ticas em **ci√™ncia de dados e minera√ß√£o de dados**, mas tamb√©m contribuir para os **Objetivos de Desenvolvimento Sustent√°vel (ODS)** da ONU, como o **ODS 9 (Inova√ß√£o e Infraestrutura)**, o **ODS 4 (Educa√ß√£o de Qualidade)** e o **ODS 10 (Redu√ß√£o das Desigualdades)**. Assim, o sistema pretende oferecer recomenda√ß√µes personalizadas que promovam maior **diversidade cultural e inclus√£o digital**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed057d11",
   "metadata": {},
   "source": [
    "Para a an√°lise e desenvolvimento deste projeto, ser√° necess√°rio utilizar um conjunto de bibliotecas do ecossistema Python, cada uma com um papel espec√≠fico no fluxo de trabalho de **an√°lise explorat√≥ria** e **constru√ß√£o do sistema de recomenda√ß√£o**.\n",
    "\n",
    "- **pandas**: essencial para manipula√ß√£o e an√°lise de dados tabulares, permitindo leitura, limpeza e transforma√ß√£o dos datasets `tmdb_5000_movies` e `tmdb_5000_credits`.  \n",
    "- **numpy**: fornece suporte para opera√ß√µes matem√°ticas e vetoriza√ß√£o, aumentando a efici√™ncia no processamento de dados.  \n",
    "- **matplotlib**: utilizada para criar visualiza√ß√µes b√°sicas, como gr√°ficos de barras, dispers√£o e histogramas.  \n",
    "- **seaborn**: complementa o matplotlib oferecendo visualiza√ß√µes estat√≠sticas mais sofisticadas e com est√©tica aprimorada.  \n",
    "- **scikit-learn**: importante para o pr√©-processamento dos dados, c√°lculo de m√©tricas de avalia√ß√£o e implementa√ß√£o de algoritmos de recomenda√ß√£o baseados em aprendizado de m√°quina.  \n",
    "- **scipy**: ser√° usada para c√°lculos matem√°ticos \n",
    "- **surprise (scikit-surprise)**: biblioteca especializada em sistemas de recomenda√ß√£o, especialmente nos modelos colaborativos como **SVD** e **KNNBasic**, permitindo comparar t√©cnicas.  \n",
    "- **lightfm**: possibilita a constru√ß√£o de sistemas de recomenda√ß√£o h√≠bridos, combinando informa√ß√µes de conte√∫do e intera√ß√µes de usu√°rios.  \n",
    "- **tensorflow / pytorch**: √∫teis caso seja necess√°rio evoluir para modelos de recomenda√ß√£o mais avan√ßados baseados em **deep learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f5ca1",
   "metadata": {},
   "source": [
    "Abaixo, ser√° realizada a importa√ß√£o das bibliotecas necess√°rias para a **an√°lise explorat√≥ria** e a **constru√ß√£o do sistema de recomenda√ß√£o**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07836b9a",
   "metadata": {},
   "source": [
    "Antes de iniciar a an√°lise, √© necess√°rio garantir que todas as bibliotecas utilizadas neste projeto estejam instaladas no ambiente.  \n",
    "Para isso, basta executar o comando abaixo no terminal ou em uma c√©lula do Jupyter Notebook (prefixado com `!`):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb00113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas para an√°lise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para sistemas de recomenda√ß√£o\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos datasets\n",
    "movies_df = pd.read_csv(\"datasets/tmdb_5000_movies.csv\")\n",
    "credits_df = pd.read_csv(\"datasets/tmdb_5000_credits.csv\")\n",
    "\n",
    "# Mostrando as primeiras linhas dos datasets\n",
    "print(\"üìå Movies Dataset:\")\n",
    "display(movies_df.head())\n",
    "\n",
    "print(\"\\nüìå Credits Dataset:\")\n",
    "display(credits_df.head())\n",
    "\n",
    "# Informa√ß√µes b√°sicas dos datasets\n",
    "print(\"\\nüîé Informa√ß√µes do Movies Dataset:\")\n",
    "print(movies_df.info())\n",
    "\n",
    "print(\"\\nüîé Informa√ß√µes do Credits Dataset:\")\n",
    "print(credits_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40fd2f4",
   "metadata": {},
   "source": [
    "Nesta etapa, ser√° realizada a **sele√ß√£o das colunas relevantes** para a constru√ß√£o do sistema de recomenda√ß√£o.  \n",
    "\n",
    "O objetivo √© reduzir o dataframe apenas √†s vari√°veis que realmente ser√£o utilizadas no modelo, garantindo maior **efici√™ncia no processamento**, evitando redund√¢ncias e mantendo o foco nos atributos mais informativos para a gera√ß√£o das recomenda√ß√µes.\n",
    "\n",
    "Al√©m disso, tamb√©m ser√£o tratados os valores nulos para diminuir o ru√≠do presente no dataset.\n",
    "\n",
    "Tamb√©m ser√° feito um **merge com o dataframe de cr√©ditos**, permitindo integrar informa√ß√µes sobre elenco e equipe t√©cnica ao conjunto de dados principal, enriquecendo assim a base para a constru√ß√£o do sistema de recomenda√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies Dataset\n",
    "\n",
    "# Filtrando somente linhas em que o filme est√° com status 'Released'\n",
    "# Utilizamos o reset_index(drop=True) para resetar os √≠ndices do DataFrame ap√≥s o filtro\n",
    "movies_df = movies_df[movies_df['status'] == 'Released'].reset_index(drop=True)\n",
    "# Agora, filtraremos apenas as colunas que nos interessam para a an√°lise\n",
    "movies_df = movies_df[['id', 'title', 'genres', 'vote_average', 'vote_count', 'popularity', 'release_date', 'overview', 'runtime', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']]\n",
    "# Tratando linhas com valores nulos\n",
    "movies_df = movies_df.dropna().reset_index(drop=True)\n",
    "# Convertendo a coluna 'release_date' para o tipo datetime\n",
    "movies_df['release_date'] = pd.to_datetime(movies_df['release_date'], errors='coerce')\n",
    "\n",
    "# Credits Dataset\n",
    "# Selecionando apenas as colunas que nos interessam\n",
    "credits_df = credits_df[['movie_id', 'cast', 'crew']]\n",
    "# Tratando linhas com valores nulos\n",
    "credits_df = credits_df.dropna().reset_index(drop=True)\n",
    "# Renomeando a coluna 'movie_id' para 'id' para facilitar o merge\n",
    "credits_df = credits_df.rename(columns={'movie_id': 'id'})\n",
    "\n",
    "# Merge dos datasets\n",
    "# Realizando o merge dos datasets 'movies_df' e 'credits_df' com base na coluna 'id'\n",
    "movies_df = movies_df.merge(credits_df, on='id')\n",
    "# Mostrando as primeiras linhas do dataset final\n",
    "print(\"\\nüìå Dataset Final ap√≥s Merge:\")\n",
    "display(movies_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc8c38",
   "metadata": {},
   "source": [
    "Neste trecho de c√≥digo √© realizado o **tratamento das colunas no formato JSON** presentes nos datasets.  \n",
    "Foram criadas fun√ß√µes auxiliares para converter as colunas que armazenam listas de dicion√°rios em **listas de valores extra√≠dos**, de forma a facilitar a manipula√ß√£o e an√°lise.  \n",
    "\n",
    "- A fun√ß√£o `parse_json_column` √© respons√°vel por percorrer colunas no formato JSON e **extrair os valores de um campo espec√≠fico** (como `name`, `iso_3166_1` ou `iso_639_1`).  \n",
    "- A fun√ß√£o `extract_director` percorre a coluna `crew` e **identifica o diretor principal** de cada filme, criando uma nova coluna `director`.  \n",
    "\n",
    "Ap√≥s o processamento, as colunas originais em JSON s√£o transformadas em listas de valores mais simples, e a coluna `crew` foi descartada, visto que sua informa√ß√£o relevante (o diretor) j√° foi extra√≠da.  \n",
    "Esse tratamento torna o dataframe mais **limpo e estruturado**, possibilitando seu uso na an√°lise explorat√≥ria e na constru√ß√£o do sistema de recomenda√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954745df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes para processar as colunas JSON\n",
    "def parse_json_column(movies_df, column_name, key='name'):\n",
    "    def extract_names(json_str):\n",
    "        if isinstance(json_str, str):\n",
    "            try:\n",
    "                list_of_dicts = ast.literal_eval(json_str)\n",
    "                if isinstance(list_of_dicts, list):\n",
    "                    return [d[key] for d in list_of_dicts if key in d]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "        return []\n",
    "    movies_df[column_name] = movies_df[column_name].apply(extract_names)\n",
    "    return movies_df\n",
    "\n",
    "# M√©todo para extrair o diretor do elenco\n",
    "def extract_director(crew_json):\n",
    "    if isinstance(crew_json, str):\n",
    "        try:\n",
    "            crew_list = ast.literal_eval(crew_json)\n",
    "            for member in crew_list:\n",
    "                if member.get('job') == 'Director':\n",
    "                    return member.get('name')\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "    return np.nan\n",
    "\n",
    "# Aplicando as fun√ß√µes para explodir as colunas JSON\n",
    "movies_df = parse_json_column(movies_df, 'genres')\n",
    "movies_df = parse_json_column(movies_df, 'keywords')\n",
    "movies_df = parse_json_column(movies_df, 'production_companies')\n",
    "movies_df = parse_json_column(movies_df, 'production_countries', key='iso_3166_1') \n",
    "movies_df = parse_json_column(movies_df, 'spoken_languages', key='iso_639_1')\n",
    "movies_df = parse_json_column(movies_df, 'cast', key='name')\n",
    "movies_df['director'] = movies_df['crew'].apply(extract_director)\n",
    "\n",
    "# Removendo a coluna 'crew' original, pois j√° extra√≠mos o diretor\n",
    "movies_df = movies_df.drop(columns=['crew'])\n",
    "\n",
    "# Mostrando as primeiras linhas do dataset ap√≥s o tratamento das colunas JSON\n",
    "print(\"\\nüìå Dataset Final ap√≥s tratamento de JSON:\")\n",
    "display(movies_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7db72",
   "metadata": {},
   "source": [
    "Abaixo, salvamos o *dataset* normalizado em um arquivo CSV, para que possamos reutiliz√°-lo novamente sem a necessidade de executar os m√©todos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.to_csv(\"datasets/movies_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebeaa6a",
   "metadata": {},
   "source": [
    "O processo de treinamento do **sistema de recomenda√ß√£o baseada em conte√∫do** consiste em transformar as **informa√ß√µes textuais dos filmes** em representa√ß√µes num√©ricas que permitam medir similaridade. Para isso, os atributos mais relevantes ‚Äî como **sinopse (overview), g√™neros, palavras-chave, elenco e diretor** ‚Äî s√£o **concatenados em um √∫nico campo de texto**. Em seguida, aplica-se a t√©cnica **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)**, que gera uma matriz esparsa onde cada filme √© representado por um vetor num√©rico que valoriza **termos mais caracter√≠sticos** e reduz a import√¢ncia de **palavras comuns**.  \n",
    "\n",
    "Com a matriz **TF-IDF** constru√≠da, o pr√≥ximo passo √© calcular a **similaridade do cosseno** entre os vetores, o que permite identificar filmes **mais pr√≥ximos em termos de conte√∫do**. Assim, dado um t√≠tulo escolhido pelo usu√°rio, o sistema compara seu vetor com todos os outros e retorna uma **lista dos filmes mais semelhantes**. Esse processo **dispensa dados expl√≠citos de usu√°rios** e permite **recomenda√ß√µes personalizadas** a partir das caracter√≠sticas intr√≠nsecas dos filmes.  \n",
    "\n",
    "Al√©m disso, tamb√©m foi criado um m√©todo que dado um texto gen√©rico de busca (em ingl√™s), o sistema compara o vetor do texto gen√©rico com o conte√∫do do filme baseado nas colunas do *dataset* normalizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7659c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes auxiliares para prepara√ß√£o dos dados\n",
    "def normalize_token(tok: str) -> str:\n",
    "    \"\"\"Normaliza um token: min√∫sculas, espa√ßos por underscores, remove caracteres especiais.\"\"\"\n",
    "    t = tok.lower().strip()\n",
    "    t = re.sub(r\"\\s+\", \"_\", t)\n",
    "    t = re.sub(r\"[^a-z0-9_\\-]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Converte uma string que representa uma lista em uma lista de tokens normalizados.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith('[') and s.endswith(']'):\n",
    "            try:\n",
    "                val = ast.literal_eval(s)\n",
    "                if isinstance(val, list):\n",
    "                    return [normalize_token(str(v)) for v in val if str(v).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        return [normalize_token(s)] if s else []\n",
    "    return []\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Limpa um texto: min√∫sculas, remove caracteres especiais, m√∫ltiplos espa√ßos.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    \"\"\"Une uma lista de tokens em uma string, ignorando valores n√£o-string ou vazios.\"\"\"\n",
    "    return \" \".join([t for t in tokens if isinstance(t, str) and t])\n",
    "\n",
    "def prepare_data(csv_path=\"datasets/movies_cleaned.csv\"):\n",
    "    \"\"\"Prepara os dados para o sistema de recomenda√ß√£o.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # listas normalizadas\n",
    "    df[\"director_list\"] = df[\"director\"].apply(lambda x: [normalize_token(x)] if isinstance(x, str) and x.strip() else [])\n",
    "    for col in [\"genres\", \"keywords\", \"cast\"]:\n",
    "        df[col + \"_list\"] = df[col].apply(parse_listish)\n",
    "\n",
    "    # texto livre\n",
    "    df[\"overview_clean\"] = df[\"overview\"].apply(clean_text)\n",
    "\n",
    "    # bolsa simb√≥lica (tokens)\n",
    "    df[\"symbolic_bag\"] = (\n",
    "        df[\"genres_list\"].apply(join_tokens) + \" \" +\n",
    "        df[\"keywords_list\"].apply(join_tokens) + \" \" +\n",
    "        df[\"cast_list\"].apply(join_tokens) + \" \" +\n",
    "        df[\"director_list\"].apply(join_tokens)\n",
    "    ).str.strip()\n",
    "\n",
    "    # campo final\n",
    "    df[\"final_text\"] = (df[\"overview_clean\"] + \" \" + df[\"symbolic_bag\"]).str.strip()\n",
    "\n",
    "    # guardamos algumas colunas √∫teis para posterior ranking\n",
    "    base_cols = [\"id\", \"title\", \"vote_average\", \"vote_count\", \"popularity\"]\n",
    "    keep = [c for c in base_cols if c in df.columns]\n",
    "    idx_df = df[keep].copy()\n",
    "    return df, idx_df\n",
    "\n",
    "def train_or_load(df, vec_path=\"datasets/tfidf_vectorizer.pkl\", mat_path=\"datasets/tfidf_matrix.npz\", idx_path=\"datasets/movies_index.csv\"):\n",
    "    \"\"\"Treina ou carrega o modelo TF-IDF e a matriz esparsa.\"\"\"\n",
    "    if os.path.exists(vec_path) and os.path.exists(mat_path) and os.path.exists(idx_path):\n",
    "        vectorizer = joblib.load(vec_path)\n",
    "        tfidf_matrix = sparse.load_npz(mat_path)\n",
    "        idx_df = pd.read_csv(idx_path)\n",
    "        return vectorizer, tfidf_matrix, idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_to_index = {t.lower(): i for i, t in enumerate(movies_df[\"title\"].astype(str))}\n",
    "vectorizer, tfidf_matrix, idx_df = train_or_load(prepare_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a012fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(query_title: str, top_n: int = 10, exclude_same: bool = True):\n",
    "    \"\"\"Recomenda filmes similares a um t√≠tulo dado.\"\"\"\n",
    "    idx = title_to_index.get(query_title.lower())\n",
    "    if idx is None:\n",
    "        raise ValueError(f\"T√≠tulo n√£o encontrado no dataset: {query_title}\")\n",
    "    sims = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).ravel()\n",
    "    order = np.argsort(-sims)\n",
    "    results = []\n",
    "    for i in order:\n",
    "        if exclude_same and i == idx:\n",
    "            continue\n",
    "        results.append((movies_df.at[i, \"title\"], float(sims[i]), int(movies_df.at[i, \"vote_count\"]), float(movies_df.at[i, \"vote_average\"])))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    rec_df = pd.DataFrame(results, columns=[\"title\", \"similarity\", \"vote_count\", \"vote_average\"])\n",
    "    return rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e405e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_text(query_text: str, top_n: int = 10, rerank_popularity: bool = True):\n",
    "    \"\"\"\n",
    "    Gera recomenda√ß√µes a partir de um texto livre.\n",
    "    - query_text: ex. \"space adventure with aliens and strong female lead\"\n",
    "    - rerank_popularity: se True, reordena suavemente por popularidade (evita filmes obscuros no topo)\n",
    "    \"\"\"\n",
    "    assert isinstance(query_text, str) and query_text.strip(), \"query_text deve ser uma string n√£o vazia.\"\n",
    "    # carregamento/treino (idempotente)\n",
    "    df, idx_df = prepare_data(\"datasets/movies_cleaned.csv\")\n",
    "    vectorizer, tfidf_matrix, _ = train_or_load(df)\n",
    "\n",
    "    # Vetoriza a consulta usando o vocabul√°rio TF-IDF treinado\n",
    "    q = clean_text(query_text)\n",
    "    q_vec = vectorizer.transform([q])\n",
    "\n",
    "    # Similaridade com todos os filmes\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix).ravel()\n",
    "\n",
    "    # Monta dataframe de resultados\n",
    "    out = idx_df.copy()\n",
    "    out[\"similarity\"] = sims\n",
    "\n",
    "    # (Opcional) re-rank por popularidade/nota com mistura linear simples\n",
    "    # Normaliza popularidade e vote_average para [0,1]\n",
    "    if rerank_popularity:\n",
    "        if \"popularity\" in out.columns:\n",
    "            pop = out[\"popularity\"].astype(float)\n",
    "            pop_norm = (pop - pop.min()) / (pop.max() - pop.min() + 1e-9)\n",
    "        else:\n",
    "            pop_norm = 0.0\n",
    "\n",
    "        if \"vote_average\" in out.columns:\n",
    "            va = out[\"vote_average\"].astype(float)\n",
    "            va_norm = (va - va.min()) / (va.max() - va.min() + 1e-9)\n",
    "        else:\n",
    "            va_norm = 0.0\n",
    "\n",
    "        # mistura: 70% similaridade + 20% popularidade + 10% nota\n",
    "        final_score = 0.7 * out[\"similarity\"].values + 0.2 * pop_norm + 0.1 * va_norm\n",
    "        out[\"final_score\"] = final_score\n",
    "        out = out.sort_values(by=[\"final_score\", \"similarity\"], ascending=False)\n",
    "    else:\n",
    "        out = out.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    cols_show = [c for c in [\"title\", \"similarity\", \"final_score\", \"vote_average\", \"vote_count\", \"popularity\", \"id\"] if c in out.columns]\n",
    "    return out[cols_show].head(top_n).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28039ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title = \"The Dark Knight\"\n",
    "rec_demo = recommend_movies(sample_title, top_n=10, exclude_same=False)\n",
    "query = \"bat costume, dark city, vigilante, high tech gadgets\"\n",
    "recs = recommend_by_text(query, top_n=10, rerank_popularity=False)\n",
    "\n",
    "print(\"Artefatos salvos em /mnt/data:\")\n",
    "print(\" - tfidf_vectorizer.pkl\")\n",
    "print(\" - tfidf_matrix.npz\")\n",
    "print(\" - movies_index.csv\")\n",
    "print(\"\\nFun√ß√£o dispon√≠vel: recommend_movies(<t√≠tulo>, top_n=10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rec_demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
